

OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBEDDING_DIMENSIONS=1536
VECTOR_INDEX_PATH=data/vector_indices
MAX_TOKENS_PER_REQUEST=8192
RPM_LIMIT=300
TIMEOUT=120.0
HNSW_M=32
HNSW_EF_CONSTRUCTION=100
HNSW_EF_SEARCH=64


NORMALIZE_EMBEDDINGS=True
USE_HNSW=True
#EMBEDDINGS_DIR=  # optional index_path
CACHE_EMBEDDINGS=True


# Hybrid Retrieval Configuration
# Copy this file to .env and fill in your values

OPENAI_API_KEY=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
OPENAI_API_BASE=https://api.openai.com/v1
BM25_K1=1.5
BM25_B=0.75

# Vector Retrieval Parameters (required)
VECTOR_API_BASE=https://api.openai.com/v1
VECTOR_API_KEY=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
VECTOR_MODEL_NAME=text-embedding-3-small
VECTOR_DIMENSIONS=1536
VECTOR_MAX_TOKENS_PER_REQUEST=8192
VECTOR_MAX_ITEMS_PER_BATCH=100
VECTOR_RPM_LIMIT=300
VECTOR_TIMEOUT=120.0
VECTOR_NORMALIZE_EMBEDDINGS=True
VECTOR_HNSW_INDEX_PATH=data/vector_indices/hnsw_text_embedding_3_small_dim1536_500.index
VECTOR_USE_HNSW=True
VECTOR_HNSW_M=32
VECTOR_HNSW_EF_CONSTRUCTION=100
VECTOR_HNSW_EF_SEARCH=64
VECTOR_EMBEDDINGS_DIR=
VECTOR_CACHE_EMBEDDINGS=True

# Retrieval Parameters (required)
RETRIEVAL_TOP_K=50

# Final Output Parameters (required)
FINAL_TOP_K_1=5
FINAL_TOP_K_2=10

# RRF Fusion Parameters (required)
USE_RRF=True
RRF_K=60
RRF_TOP_K=50

# ============================================================================
# LLM Re-ranking Configuration - OPTIMIZED VALUES
# ============================================================================
# Based on performance analysis:
# - Current: 45-67s response time, 20% missing items
# - Optimization: Reduce prompt size, increase output buffer

# LLM model for re-ranking
# Options: gpt-4o-mini (faster, cheaper), gpt-4o (slower, better quality)
LLM_MODEL=gpt-4o-mini

# Maximum tokens per item in the prompt (REDUCED from 200 to 120 for faster response)
# Each item text is truncated to this many tokens to reduce prompt size
# 120 tokens ≈ 90-120 words, usually sufficient for item description
LLM_MAX_TOKENS_PER_ITEM=120

# Maximum context window tokens for the LLM model
# gpt-4o-mini: 128000, gpt-4o: 128000, gpt-3.5-turbo: 16385
LLM_MAX_CONTEXT_TOKENS=128000

# Reserved output tokens (for JSON response structure)
# Increased from 8000 to ensure enough space
LLM_RESERVED_OUTPUT_TOKENS=10000

# Estimated tokens per item in the output JSON (INCREASED from 60 to 80)
# Each item output: {"item_id": "...", "score": X} ≈ 50-80 tokens
# Increased to account for variations and ensure completeness
LLM_TOKENS_PER_ITEM_OUTPUT=80

# Sleep time between LLM API calls (seconds, for rate limiting)
# 0.0 = no sleep, use if you have high rate limits
LLM_SLEEP=0.0

# Timeout for LLM API calls (seconds) - INCREASED from 120 to 180
# Large prompts (7000+ tokens) may take 60-90 seconds, so we need more buffer
LLM_TIMEOUT=180.0
LLM_RERANK_BATCH_SIZE=10
# ============================================================================
# Two-Stage Re-ranking Configuration (Optional)
# ============================================================================
# Enable two-stage re-ranking: first use lightweight reranker (50→20), then LLM (20→top-5/10)
# This can reduce latency and cost by 5-10x with minimal hit rate loss
USE_TWO_STAGE_RERANKING=True

# Lightweight reranker model (required if USE_TWO_STAGE_RERANKING=True)
# Options: 
#   - BAAI/bge-reranker-base (recommended, better quality)
#   - BAAI/bge-reranker-small (faster, smaller model)
LIGHTWEIGHT_RERANKER_MODEL=BAAI/bge-reranker-small

# Number of items after first-stage reranking (input to LLM)
# Default: 20 (compresses 50 items to 20 before LLM re-ranking)
LIGHTWEIGHT_RERANKER_TOP_K=20

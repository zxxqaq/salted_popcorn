[retriever]
type = "bm25"
items_path = "data/test/500_items.csv"
params.k1 = 1.5
params.b = 0.75

[data]
test_path = "data/test/test_query_new.csv"
queries_path = "data/test/10_queries.csv"

[evaluation]
ks = [5, 10]
metrics = ["precision", "recall", "ndcg", "mrr", "coverage"]
min_relevance = 6.0  # Only consider scores >= 1.0 as relevant for Precision/Recall/MRR (on 0-10 scale)
min_relevance_ndcg = 0.0  # Include all scores (including 0) for NDCG calculation

[output]
dir = "artifacts/eval_runs"
tag = "bm25_with_llm_scores"


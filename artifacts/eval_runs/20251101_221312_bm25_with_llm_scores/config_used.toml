[retriever]
type = "bm25"
items_path = "data/test/500_items.csv"
params.k1 = 1.5
params.b = 0.75

[data]
test_path = "data/test/test_query.csv"
queries_path = "data/test/10_queries.csv"

[evaluation]
ks = [5, 10]
metrics = ["precision", "recall", "ndcg", "map", "mrr"]
min_relevance = 1.0  # Only consider scores >= 1.0 as relevant (on 0-10 scale)

[output]
dir = "artifacts/eval_runs"
tag = "bm25_with_llm_scores"


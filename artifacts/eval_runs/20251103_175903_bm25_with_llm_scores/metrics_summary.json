{
  "config": {
    "retriever": {
      "type": "bm25",
      "items_path": "data/test/500_items.csv",
      "params": {
        "k1": 1.5,
        "b": 0.75
      }
    },
    "data": {
      "test_path": "data/test/test_query_new.csv",
      "queries_path": "data/test/10_queries.csv"
    },
    "evaluation": {
      "ks": [
        5,
        10
      ],
      "metrics": [
        "precision",
        "recall",
        "ndcg",
        "mrr",
        "coverage"
      ],
      "min_relevance": 6.0,
      "min_relevance_ndcg": 0.0
    },
    "output": {
      "dir": "artifacts/eval_runs",
      "tag": "bm25_with_llm_scores"
    }
  },
  "retriever": {
    "type": "bm25",
    "items_path": "data/test/500_items.csv",
    "params": {
      "k1": 1.5,
      "b": 0.75
    }
  },
  "query_count": 10,
  "skipped_queries": [],
  "metrics": {
    "precision@5": 0.28,
    "recall@5": 0.049734909115161215,
    "ndcg@5": 0.19069470516363424,
    "precision@10": 0.2,
    "recall@10": 0.061975745748321945,
    "ndcg@10": 0.1810531461033992,
    "reciprocal_rank": 0.5491666666666666
  },
  "timing": {
    "query_count": 10,
    "skipped_query_count": 0,
    "avg_latency_ms": 0.2482125535607338,
    "median_latency_ms": 0.24514587130397558,
    "p95_latency_ms": 0.32677912386134267,
    "p99_latency_ms": 0.32878935569897294,
    "min_latency_ms": 0.13808393850922585,
    "max_latency_ms": 0.3292919136583805
  },
  "coverage": {
    "coverage@5": 0.8,
    "coverage@10": 0.9
  }
}
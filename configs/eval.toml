[retriever]
type = "bm25"
items_path = "data/raw/5k_items_curated.csv"
params.k1 = 1.5
params.b = 0.75

[data]
test_path = "data/test.csv"

[evaluation]
ks = [5, 10, 20]
metrics = ["precision", "recall", "ndcg", "map", "mrr"]
min_relevance = 0.5

[output]
dir = "artifacts/eval_runs"
tag = "bm25_baseline"

